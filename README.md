# Phenotyping-and-Genotyping-Alzheimers-Diseases

I used machine learning method to genotype the microarray data of Alzheimer’s diseases downloaded from GEO website. Alzheimer's disease (AD) is a progressive neural disorder that causes a continuous decline in thinking, behavioral and social skills. People with AD lose their ability to function independently in their later life. To help improve the life quality of AD patients, it is necessary to identify new biomarkers for AD since they enable us to have a deeper understanding of AD and facilitate drug and treatment development. From the research last semester, I read many papers related to genetic association with Alzheimer’s diseases and identified the genes of interest to prepare for this research project. 
The microarray data consist of has 230 samples and 39,283 features for each brain compartment for three different compartments of the brain with two labels, Alzheimer’s patients, and control patients. I focused on one compartment, prefrontal cortex. I extracted the description file of all the genes in the list from the soft file on the GEO website. Then, I analyzed the phenotype data provided in the data set. Then, I focused on the genetic microarray data.  Normalization is the process of scaling raw count values to account for other factors. In this way the expression levels are more comparable between and/or within samples. I also performed the differential expression analysis to identify deferentially expressed genes. I tried to normalize the data to identify the differentially expressed genes and realized that data is already normalized. To identify significant features, T-test and Benjamin and Hochberg (false discovery rate), were used to calculate the p-value and FDR to determine the DEGs among patients with AD and normal. I set the principal standard p < 0.05 to acquire DEGs that are significant from the dataset. Then, I used the identified genes to perform GO enrichment analysis and to only include the significant genes for the later analysis. I was unable to perform log2 fold change due to the data was already normalized, so I just performed a ratio between two data sets and set the threshold as the mean of change of the dataset to identify DEG. In total, I identified 427 DEG.

After the identification of DEG, I used random forest to genotyping them because random forest is an ensemble learning method based on the construction of many classification trees and the main benefits of the method are its robustness against overfitting, the easy interpretation of the model and its excellent performance on high dimensional data. The data was split into 70% training and 30% testing to train the model and then I omitted features with feature importance equals to 0 and fitted the new data to the model again. The accuracy is 0.9689. In order to do the validation for the model, the out-of-bag (OOB) score is calculated for each built tree. The OOB samples are the samples that were not selected to build a particular tree. Once one tree is built with the bootstrapped samples, the OOB score can be computed as the mean prediction accuracy from those OOB samples, and the final OOB score for the entire forest is the average of all those scores to get an estimate for how accurate the random forest performs, which is essentially leave-one-out cross validation. Then, I am going to use cross-validation to validate the model.
